{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport keras\nprint(keras.__version__)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"HW = 'humpback-whale-identification'\n# TRAIN = '../input/humpback-whale-identification/train/'\nTRAIN_CROPPED = \"whales-cropped/cropped_train/cropped_train/\"\nTRAIN_CROPPED_IN = '../input/' + TRAIN_CROPPED\n\n# TEST = '../input/humpback-whale-identification/test/'\nTEST_CROPPED = \"whales-cropped/cropped_test/cropped_test/\"\nTEST_CROPPED_IN = '../input/' + TEST_CROPPED\n\nLABELS = '../input/humpback-whale-identification/train.csv'\nSAMPLE_SUB = '../input/humpback-whale-identification/sample_submission.csv'\n\ntrain = pd.read_csv(LABELS)\nprint(\"With new_whale:\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1dea1195c936c9dd22093483b7a4b32746a978b"},"cell_type":"code","source":"MODEL_F = 'Model_InceptionResNetV2.h5'\nWEIGHTS_F = 'Weights_InceptionResNetV2.h5'\nMODEL = '../input/InceptionResNetV2/'+ MODEL_F\nWEIGHTS = '../input/InceptionResNetV2/'+ WEIGHTS_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8226025070bee28da643e57f925a6c3c075f63b"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2094ad7d97a11ecd4f7c7dd35cc2cf3fa8ae59"},"cell_type":"code","source":"import random \nfrom IPython.display import Image\nprint(\"Example whale image\")\n\n#show sample image\nname = random.choice(train['Image'])\nprint(name)\nImage(filename = TRAIN_CROPPED_IN + name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9cd0a1227409ddd9f0154cd51136b84615866d5"},"cell_type":"code","source":"criteria = train['Id'] != 'new_whale'\nwhales_train = train[criteria]\n    \nprint(\"Without new_whale:\")\nwhales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10d3b64626005f5f0b1714bed949c7178674b6f2"},"cell_type":"code","source":"unique_labels = np.unique(whales_train.Id.values)\nlabels_list = unique_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe40c404327ac23b61cf2e1493dd033ece5b7d71"},"cell_type":"code","source":"whales_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f900184097fc35e7feedde2cc05e5cd00c84d9a"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras import layers\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# from keras.applications.imagenet_utils import preprocess_input\n# from keras.applications.resnet50 import ResNet50, preprocess_input\n# from keras.applications.xception import Xception, preprocess_input\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n\nfrom keras.losses import binary_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, GlobalAveragePooling2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.models import Model\nfrom keras.metrics import top_k_categorical_accuracy\n\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom PIL import Image\nimport gc\nimport warnings\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c60eeebdd134f57556c6e7ef7f7e8151801cae78"},"cell_type":"code","source":"IMAGE_HEIGHT = 128\nIMAGE_WIDTH = 128\nIMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10dbfd1a18557125abc46b6aec885a66a4dcd62e"},"cell_type":"code","source":"CLASSES = 5004\nEPOCHS = 5\nBATCH_SIZE = 32\n\ndef top_5_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=5)\n\n# setup model\nbase_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape = IMAGE_SHAPE)\n\nx = base_model.output\nx = GlobalAveragePooling2D(name='avg_pool')(x)\nx = Dropout(0.4)(x)\npredictions = Dense(CLASSES, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n   \n# transfer learning\nfor layer in base_model.layers:\n  layer.trainable = True\n      \nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy', 'mae', top_5_accuracy])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a1a6be045ecb003b33b899ed11ebedbde4befd"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df0c0f6c2811381f2dcdb25056975b8aaa15714f"},"cell_type":"markdown","source":"# Load model and weights from disc <a name=\"flow\"></a>\n____"},{"metadata":{"trusted":true,"_uuid":"a828703f48eb3defa1ed5d5b7ec058b5c2e3f1ac"},"cell_type":"code","source":"# from keras.models import load_model\n\n# # returns a compiled model\n# # identical to the previous cell\n# model = load_model(MODEL)\n# print(\"Loaded model architecture from disk\")\n# gc.collect()\n\n# model.load_weights(WEIGHTS)\n# print(\"Loaded model weights from disk\")\n# model.summary()\n\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9a137230e1dc5314019fd6bb34447a9fce363e0"},"cell_type":"markdown","source":"# Train with ImageDataGenerator and flow_from_dataframe\n____"},{"metadata":{"trusted":true,"_uuid":"72526972714865c274c2a67f7567f6ec2f3cbf31"},"cell_type":"code","source":"from keras.callbacks import LambdaCallback, ModelCheckpoint\n\nROTATE = 20\nSEED = 28\nBATCH_SIZE = 100\n\ngc.collect()\n\nbatch_gc_callback = LambdaCallback(\n    on_epoch_begin=lambda epoch,logs: gc.collect())\n\ncheckpointer = ModelCheckpoint(filepath='weights.hdf5', \n                               verbose=1, save_best_only=True)\n\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rescale=1./255,\n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=whales_train, \n    subset = \"training\",\n    directory=TRAIN_CROPPED_IN, \n    x_col=\"Image\", \n    y_col=\"Id\", \n    has_ext=True, \n    seed = SEED,\n    color_mode= \"rgb\",\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), \n    batch_size=BATCH_SIZE, \n    class_mode='categorical')\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a2e9a4256b37ed71bef2d5ac491e77db4515860"},"cell_type":"markdown","source":"# Visualize augmented data <a name=\"flow\"></a>\n____"},{"metadata":{"trusted":true,"_uuid":"52aa43e0c154603af6d5b82515915a2691dbbcb9"},"cell_type":"code","source":"from skimage.io import imread\nimport PIL.Image as pimage\n\ndef plot_images(images_names, path):\n    fig, m_axs = plt.subplots(1, len(images), figsize = (20, 10))\n    #show the images and label them\n    for ii, c_ax in enumerate(m_axs):\n        img = imread(os.path.join(path,images_names[ii][0]))\n        c_ax.imshow(img)\n        c_ax.set_title(images_names[ii][1])\n\ndef plot_loaded_images(images_loaded, labels):\n    fig, m_axs = plt.subplots(1, len(images_loaded), figsize = (20, 10))\n    #show the images and label them\n    for ii, c_ax in enumerate(m_axs):\n        img = pimage.fromarray(images_loaded[ii], \"RGB\")\n        c_ax.imshow((images_loaded[ii] + 1) / 2)\n#         c_ax.set_title(labels[ii])\n        \n#get the first 5 whale images\nimages = [(whale_img, whale_label) for (whale_img, whale_label) in zip(whales_train.Image[:5], whales_train.Id[:5])]\nplot_images(images, TRAIN_CROPPED_IN)\n\nx_batch, y_batch = next(train_generator)\nplot_loaded_images(x_batch[:5], y_batch[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c54e22957b27411bb5d40a974d975c570e39ca"},"cell_type":"markdown","source":"# Learning rate finder <a name=\"flow\"></a>\n____"},{"metadata":{"trusted":true,"_uuid":"ee7699656b9596ba262478209de4a388d584147b"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.callbacks import Callback\n\n\nclass LRFinder(Callback):\n    '''\n    A simple callback for finding the optimal learning rate range for your model + dataset.\n\n    # Usage\n        ```python\n            lr_finder = LRFinder(min_lr=1e-5,\n                                 max_lr=1e-2,\n                                 steps_per_epoch=np.ceil(epoch_size/batch_size),\n                                 epochs=3)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n\n            lr_finder.plot_loss()\n        ```\n\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`.\n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient.\n\n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: https://arxiv.org/abs/1506.01186\n    '''\n\n    def __init__(self, min_lr=1e-5, max_lr=1e-1, steps_per_epoch=None, epochs=None, beta=.98):\n        super().__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        self.beta = beta\n        self.lr_mult = (max_lr/min_lr)**(1/steps_per_epoch)\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        x = self.iteration / self.total_iterations\n        return self.min_lr * (self.lr_mult**self.iteration)\n\n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        self.best_loss = 1e9\n        self.avg_loss = 0\n        self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], []\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.min_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(\" Epoch ended learning rate: \" + str(K.get_value(self.model.optimizer.lr)))\n        \n    def on_epoch_begin(self, epoch, logs=None):\n        print(\" Epoch start learning rate: \" + str(K.get_value(self.model.optimizer.lr)))\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n        \n        #addition\n        loss = logs.get('loss')\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss\n        smoothed_loss = self.avg_loss / (1 - self.beta**self.iteration)\n        # Check if the loss is not exploding\n        if self.iteration>1 and smoothed_loss > self.best_loss * 1.5:\n            self.model.stop_training = True\n            return\n        if smoothed_loss < self.best_loss or self.iteration==1:\n            self.best_loss = smoothed_loss\n        \n        lr = self.clr()\n        self.losses.append(loss)\n        self.smoothed_losses.append(smoothed_loss)\n        self.lrs.append(lr)\n        self.iterations.append(self.iteration)\n        \n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        K.set_value(self.model.optimizer.lr,lr)\n\n#     def plot_lr(self):\n#         '''Helper function to quickly inspect the learning rate schedule.'''\n#         plt.plot(self.history['iterations'], self.history['lr'])\n#         plt.yscale('log')\n#         plt.xlabel('Iteration')\n#         plt.ylabel('Learning rate')\n#         plt.show()\n\n    def plot_lr(self):\n        plt.xlabel('Iterations')\n        plt.ylabel('Learning rate')\n        plt.plot(self.iterations, self.lrs)\n        plt.yscale('log')\n        plt.show()\n\n    def plot_loss(self):\n        '''Helper function to quickly observe the learning rate experiment results.'''\n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')\n        plt.show()\n    \n    def plot(self, n_skip=10):\n        plt.ylabel('Loss')\n        plt.xlabel('Learning rate (log scale)')\n        plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5])\n        plt.xscale('log')\n        \n    def plot_loss2(self):\n        plt.ylabel('Losses')\n        plt.xlabel('Iterations')\n        plt.plot(self.iterations[10:], self.losses[10:])\n        \n    def plot_smoothed_loss(self, n_skip=6):\n        plt.ylabel('Smoothed Losses')\n        plt.xlabel('Learning rate (log scale)')\n        plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5])\n        plt.xscale('log')\n        \n    def plot_loss_change(self, sma=1, n_skip=20, y_lim=(-0.01,0.01), should_lim_y = False):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sched - learning rate scheduler, an instance of LR_Finder class.\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip - number of batches to skip on the left.\n            y_lim - limits for the y axis.\n        \"\"\"\n        derivatives = [0] * (sma + 1)\n        for i in range(1 + sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"d/loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip:], derivatives[n_skip:])\n        plt.xscale('log')\n        if should_lim_y:\n            plt.ylim(y_lim)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f73c2c8f84ee04648376a0e96e6ec3d40825622"},"cell_type":"markdown","source":"# Train <a name=\"flow\"></a>\n____"},{"metadata":{"trusted":true,"_uuid":"5eaf4f5acb9d85cfcf9d9c1aa2968cb85e05dfa8"},"cell_type":"code","source":"# fits the model on batches with real-time data augmentation:\nSTEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n\nlr_finder = LRFinder(min_lr=1e-6,\n                     max_lr=1,\n                     steps_per_epoch=np.ceil(train_generator.n//train_generator.batch_size),\n                     epochs=EPOCHS)\n\n# Train the loaded model for more epochs\nhistory = model.fit_generator(generator=train_generator,\n                              steps_per_epoch=STEP_SIZE_TRAIN,\n                              epochs=EPOCHS,\n                              callbacks=[batch_gc_callback, lr_finder])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10a837ad272c61eff75b01815ad138f1353dc793"},"cell_type":"code","source":"lr_finder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7868cee02d73a411e005f43509bd5964467a5bc"},"cell_type":"code","source":"lr_finder.plot_smoothed_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2649f93a514241bee3a403e9f7221c370c6f7230"},"cell_type":"code","source":"lr_finder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6489103dfab8442fd80c0977afd780a2d96d4b3"},"cell_type":"code","source":"lr_finder.plot_loss_change(sma=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1e3d0131841426dac876bd4f58ab976f18dfde0"},"cell_type":"code","source":"lr_finder.plot_loss_change(sma=50, y_lim=(-0.02,0.01), should_lim_y=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adbc27074f20f494670395569611c751963d2140"},"cell_type":"markdown","source":"# Plot train results <a name=\"flow\"></a>\n____"},{"metadata":{"trusted":true,"_uuid":"4803d5306931ad157b9fb9bf3923166d8fd68a0c"},"cell_type":"code","source":"def plot_accuracy(history, should_plot_val = False, should_plot_top5 = False):\n    acc = history.history['acc']\n    l1 = plt.plot(acc, label='acc')\n    \n    if should_plot_val:\n        val_acc = history.history['val_acc']\n        l2 = plt.plot(val_acc, label='val_acc')\n    \n    if should_plot_top5:\n        top5_acc = history.history['top_5_accuracy']\n        l3 = plt.plot(top5_acc, label='top_5_accuracy')\n        \n    plt.legend(loc=2, fontsize=\"small\")\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.show()\n    \ndef plot_loss(history, should_plot_val = False):\n    loss = history.history['loss']\n    l1 = plt.plot(loss, label='loss')\n    \n    if should_plot_val:\n        val_loss = history.history['val_loss']\n        plt.plot(val_loss, label='val_loss')\n        \n    plt.legend(loc=2, fontsize=\"small\")\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.show()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2642d18e30e2731f7d4750ba6fd01ec11188dd92"},"cell_type":"code","source":"plot_accuracy(history, False, True)\nplot_loss(history, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5c295af0029be05502479dc4037dba7aaddd1cb"},"cell_type":"code","source":"model.save(MODEL_F)\nprint(\"Saved model architecture to disk\")\nmodel.save_weights(WEIGHTS_F)\nprint(\"Saved model weights to disk\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}